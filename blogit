#!/usr/bin/env python3
import sys
sys.path.insert(0, 'libs/')
import os
import json
from datetime import datetime, timedelta
import re, shutil, gzip, functools
import urllib, urllib.request; urlopen = lambda x: json.loads(urllib.request.urlopen(x).read().decode('utf-8'))

import pystache
import baker
import markdown2 as markdown

from pyatom import AtomFeed
current_date = datetime.now().strftime('%Y-%m-%d')
daydelta = timedelta(days=1)

def loadConfigs(path=os.getcwd()):
    fname = os.path.join(path, 'config.json')
    if not os.path.isfile(fname):
        return None
    return json.load(open(fname))

config = loadConfigs()


def saveto(fres, addflag = ''):
    dname = os.path.dirname(fres)
    try:
        os.makedirs(dname)
    except:
        pass
    return open(fres, 'w' + addflag)


def tags_cludge(tag):
    c = tag.group(1)
    if c == '*':
        tag = '<b style="font-size: 1.6em; color: yellow;">✭</b>'
    else:
        tag = "<small>#" + c + "</small>"
    return tag


def makeHtml(inbound):
    res = []
    for line in inbound.split('\n'):
        line = line.strip()
        if line.startswith('>>') and line.endswith('<<'):
            line = line[2:-2].strip()
            if line.startswith('!['):
                # inline image!
                tp, url = line[2:-1].split('](')
                if tp == 'center':
                    line = '<div style="text-align: center"><img src="' + \
                    url + \
                    '" style="float: none" /></div>'
            else:
                raise Error('Unknown inline')
        res.append(line)
    res = '\n'.join(res)

    res = markdown.markdown(res, extras=["cuddled-lists"])

    res = re.sub(' \#([a-zA-Z0-9\*]+)', tags_cludge, res)
    res = re.sub('\{([a-zA-Z0-9\*]+)\}', tags_cludge, res)
    return res

def extract_numbers(s):
    s = s.replace('.md', '')
    num = ''.join((x for x in s if x.isdigit()))
    alpha = ''.join((x for x in s if x.isalpha()))
    return num + '-' + alpha

def _perversed_sort(x,y):
    x = extract_numbers(x)
    y = extract_numbers(y)
    return -1 if x > y else 1 if x < y else 0

def perversed(vd):
    "Sort dictionary in my own way, aha"
    return sorted(vd, key=functools.cmp_to_key(_perversed_sort))


class Index(dict):
    def __init__(self, filename=None, ro=False, *args, **kwds):
        self.filename = filename if filename else config['local']['index']
        self.ro = ro
        if os.path.isfile(self.filename):
            self.load(self.filename)
        dict.__init__(self, *args, **kwds)
        if 'idx' not in self:
            self['idx'] = []

    def load(self, fd):
        try:
            return self.update(json.load(open(fd, 'r')))
        except Exception:
            pass

    def sync(self):
        'Write dict to disk'
        if self.ro:
            return
        with saveto(self.filename) as fl:
            json.dump(self, fl, ensure_ascii=False, indent=True)

    def close(self):
        self.sync()

    def __enter__(self):
        return self

    def __exit__(self, *exc_info):
        self.close()


class Entry:
    def __init__(self, page, empty=False):
        self.is_index = False
        post_path = config['local']['posts']
        self.fullpath = os.path.join(post_path, page)
        if '.' not in self.fullpath:
            self.fullpath = self.fullpath + '.md'
        self.page = page
        self.title = None
        self.cdate = os.path.getmtime(self.fullpath)
        self.fdate = self.cdate
        if not empty and os.path.isfile(self.fullpath):
            self.load()

    def parse(self):
        pass

    def load(self):
        self.content = open(self.fullpath).read()
        self.parse()


class MarkDownEntry(Entry):
    def parse(self):
        if self.content.startswith('---'):
            headers, rest = self.content[3:].split('---', 1)
            self.headers = {}
            for key, value in (line.strip().split(': ', 1)
                                for line in headers.split('\n')
                                        if not line.startswith('#') and ':' in line):
                self.headers[key.lower()] = value
            self.content = rest
            if 'title' in self.headers:
                self.title = self.headers['title']
            if 'date' in self.headers:
                self.fdate = self.headers['date']
        elif self.content.startswith('# '):
            title, rest = self.content.split('\n', 1)
            hashmash, self.title = title.split(' ', 1)
            self.content = rest
        else:
            title, rest = self.content.split('\n', 1)
            self.title = title
            self.content = rest
        self.content = makeHtml(self.content)
        self.result = os.path.join(config['local']['results']['posts'],
                                                 self.page, 'index.html')
        self.permalink = config['site'] + config['local']['results']['site'] + self.page


    def do(self, template):
        return self.do_to(template, self.result)

    def do_to(self, template, fpath):
        res = pystache.render(open(template, 'r').read(),
            {
                'conf': config, 'title': self.title,
                'cdate': self.cdate, 'fdate': self.fdate,
                'content': self.content.replace('\n', ' '),
                'page': self.page,
                'permalink': self.permalink
            })
        with saveto(fpath) as fl:
            fl.write(res)


class Archive:
    def __init__(self, idx):
        self.idx = idx
        self.results = os.path.join(config['local']['results']['archive'], 'index.html')

    def do(self, template):
        items = []
        for x in perversed(self.idx['idx']):
            items.append({
                'fname': self.idx[x]['permalink'],
                'subtitle': self.idx[x]['title'],
                })
        res = pystache.render(open(template, 'r').read(),
            {
                'conf': config,
                'title': 'Archive',
                'items': items,
                'permalink': config['site'] + '/archive'
            })
        with saveto(self.results) as fl:
            fl.write(res)


class RSS:
    def __init__(self, idx):
        self.idx = idx
        self.results = os.path.join(config['local']['results']['feed'])

    def do(self):
        feed = AtomFeed(title=config['title'],
                    feed_url=config['site'] + "feed",
                    url=config['site'][:-1] if config['site'].endswith('/') else config['site'],
                    author=config['author'])
        for x in list(perversed(self.idx['idx']))[:10]:
            page = MarkDownEntry(x)
            feed.add(title=self.idx[x]['title'],
                 content=page.content,
                 content_type="html",
                 author=config['author'],
                 url=self.idx[x]['permalink'],
                 updated=datetime.fromtimestamp(self.idx[x]['cdate'])
            )
        feedout = gzip.compress(bytes(feed.to_string(), 'utf-8'))
        with saveto(self.results, 'b') as fl:
            fl.write( feedout )

def get_true_page(page = current_date):
    if 'yesterday' in page:
        page = page.replace('yesterday', (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d'))
    elif 'tomorrow' in page:
        page = page.replace('tomorrow', (datetime.now() + timedelta(days=1)).strftime('%Y-%m-%d'))
    elif 'today' in page:
        page = page.replace('today', datetime.now().strftime('%Y-%m-%d'))
    if '.' in page:
        page = page.rsplit('.', 1)[0]
    return page

@baker.command(
    shortopts={
            "overwrite": "o",
            "edit": "e"},
    params={"page":         "page name",
            "overwrite":    "overwrite page even if exists",
            "edit":         "open text editor if page created"}
    )
def entry(page=current_date, overwrite=False, edit=False):
    """create and edit new blog entry"""
    path = config['local']['posts']
    page = get_true_page(page)

    fullpath = os.path.join(path, page + '.md')
    if os.path.isfile(fullpath):
        if not overwrite and not edit:
            sys.stderr.write('File %s already exists.\n' % fullpath)
            return
    else:
        with saveto(fullpath) as fl:
            fl.write('# Title\n\n-----\n\n## Для всех\n## На грани\n## Для гиков\n## Разное\n\n')
    if edit:
        os.system(config['local']['editor'] % fullpath)
    else:
        sys.stdout.write(fullpath + '\n')
    return

@baker.command(
    shortopts={
            "page": "p",
            "fetch": "F"},
    params={"page":         "page name",
            "fetch": "fetch draft from default source"}
    )

def draft(page=current_date, fetch = False):
    """create and edit draft"""
    path = config['local']['drafts']
    page = get_true_page(page)

    fullpath = os.path.join(path, page + '.md')
    template = '# Title\n\n-----\n\n## Для всех\n## На грани\n## Для гиков\n## Разное\n\n'
    if fetch:
        template = source()
    if not os.path.isfile(fullpath):
        with saveto(fullpath) as fl:
            fl.write(template)
    sys.stdout.write(fullpath + '\n')
    os.system(config['local']['editor'] % fullpath)
    return

@baker.command(
    shortopts={
            "page": "p"},
    params={"page":         "page name",}
    )
def publish(page=current_date):
    """publish draft to usual post"""
    srce_path = config['local']['drafts']
    dest_path = config['local']['posts']
    page = get_true_page(page)

    s_fullpath = os.path.join(srce_path, page + '.md')
    d_fullpath = os.path.join(dest_path, page + '.md')
    if not os.path.isfile(s_fullpath):
        sys.stderr.write('Draft `' + page + "' not found\n")
        return
    if os.path.isfile(d_fullpath):
        sys.stderr.write('Draft `' + page + "' cannot be unpublished because this page already on air\n")
        return
    shutil.move(s_fullpath, d_fullpath)
    sys.stdout.write('Page `' + page + "' published\n")
    return


@baker.command(
    shortopts={"page":  "p",
               "force": "f"},
    params={"page":         "page name",
            "force":    "force regen page"}
    )
def gen(page=current_date, force=False):
    """generate a static .html for a page"""
    page = get_true_page(page)
    fpath = os.path.join(config['local']['posts'], page + '.md')
    if not force:
        with Index(ro=True) as idx:
            if page in idx and os.path.getmtime(fpath) <= idx[page]['cdate']:
                return
    entry = MarkDownEntry(page)
    with Index(ro=True) as idx:
        if page not in idx['idx']:
            entry.is_index = True
        else:
            last = None
            for x in perversed(idx['idx']):
                if 'draft' not in idx[x] or idx[x]['draft'] == False:
                    last = x
                    break
            if last == page:
                entry.is_index = True
    entry.do(config['local']['templates']['post'])
    sys.stdout.write('File %s writen\n' % entry.result)
    if entry.is_index:
        entry.do_to(config['local']['templates']['post'],
                    config['local']['results']['path'] + 'index.html')
        sys.stdout.write('Page ' + page + ' also is index\n')
    with Index() as idx:
        idx[page] = dict(
            permalink=entry.permalink,
            title=entry.title,
            cdate=entry.cdate,
            fdate=entry.fdate,
        )
        if page not in idx['idx']:
            idx['idx'].append(page)


@baker.command
def archive():
    '''regen /arhive page'''
    f = Archive(Index(ro=True))
    f.do(config['local']['templates']['archive'])
    sys.stdout.write('Archive ' + f.results + ' saved\n')


@baker.command
def feed():
    '''regen /feed page'''
    f = RSS(Index(ro=True))
    f.do()
    sys.stdout.write('Atom feed ' + f.results + ' saved\n')

@baker.command
def regenCSS():
    '''regen .less files to style.css'''
    cwd = os.getcwd()
    os.chdir(config['local']['templates']['path'] + 'bootstrap/less')
    os.system("lessc -x style.less | gzip -c -9 > " + \
                cwd + '/' + config['local']['results']['path'] + 'style.css')
    os.chdir(cwd)

@baker.command(
    shortopts={
            "wipe": "w",
            "force": "f",
            "noindex": "n",
            "andsync": "s"},
    params={"wipe":         "wipe out current index",
            "force":    "force regen pages",
            "noindex": "do not regen feed and archives",
            "andsync": "execute `sync` command right after finish"}
    )
def regen(wipe=False, force=False, noindex=False, andsync=False):
    """regenerate index and all pages"""
    if wipe:
        with Index() as idx:
            idx['idx'] = []
    for item in os.listdir(config['local']['posts']):
        gen(page=item, force=force)
    if not noindex:
        regenCSS()
        archive()
        feed()
    if andsync:
        sync()


@baker.command
def sync():
    """syncronize results tree with s3 server"""
    cwd = os.getcwd()
    os.chdir(config['local']['results']['path'])
    os.system("s3cmd --no-preserve --recursive --exclude=feed --exclude=style.css sync * " + config['s3']['bucket'])
    os.system("s3cmd --no-preserve --add-header=Content-Encoding:gzip sync -m 'text/xml' sync feed " + config['s3']['bucket'])
    os.system("s3cmd --no-preserve --add-header=Content-Encoding:gzip sync style.css " + config['s3']['bucket'])
    os.chdir(cwd)


@baker.command
def server(port=8000):
    """run a local http server in results tree"""
    cwd = os.getcwd()
    os.chdir(config['local']['results']['path'])
    import http.server
    import socketserver
    class GzippedResourcesAwareHandler(http.server.SimpleHTTPRequestHandler):
        def do_GET(self):
            if self.path == '/style.css'\
                or self.path == '/feed':
                    self.send_header("Content-Encoding", "gzip")
            super(GzippedResourcesAwareHandler, self).do_GET()
    Handler = GzippedResourcesAwareHandler
    httpd = socketserver.TCPServer(("", port), Handler)
    print("serving at port", port)
    httpd.serve_forever()
    os.chdir(cwd)

baker.run()
